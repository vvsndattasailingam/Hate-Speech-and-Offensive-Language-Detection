{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d074efab",
   "metadata": {},
   "source": [
    "\n",
    "# Hate Speech and Offensive Language Detection Project\n",
    "\n",
    "This project is part of the **Natural Language Processing (NLP)** coursework and focuses on detecting hate speech and offensive language in social media posts. The project leverages NLP techniques for preprocessing and feature extraction, alongside machine learning models for classification and performance evaluation.\n",
    "\n",
    "## Objectives\n",
    "1. Perform text preprocessing using NLP techniques.\n",
    "2. Visualize data to understand key patterns and trends.\n",
    "3. Apply multiple machine learning models for classification, including Logistic Regression, Random Forest, SVM, and Neural Networks.\n",
    "4. Compare model performances using metrics like precision, recall, F1-score, and confusion matrix.\n",
    "5. Gain insights into model strengths and identify the best-performing approach for text classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad8b58",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset Loading and Exploration\n",
    "\n",
    "We start by loading the dataset and performing an initial exploration to understand its structure, key attributes, and any preprocessing requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761377b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset (Replace 'path_to_file.csv' with your actual dataset path)\n",
    "data = pd.read_csv('path_to_file.csv')\n",
    "\n",
    "# Display dataset overview\n",
    "data.info()\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376f9f6",
   "metadata": {},
   "source": [
    "\n",
    "### Data Visualization\n",
    "\n",
    "We will visualize the class distribution and identify patterns in the dataset using various plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc3749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot class distribution\n",
    "sns.countplot(x='label', data=data)  # Replace 'label' with the actual label column name\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46560b7",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preprocessing with NLP Techniques\n",
    "\n",
    "This step involves cleaning the text data and preparing it for feature extraction. We will use NLTK for tokenization, stemming, and stopword removal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2028d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert to lowercase and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    # Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# Apply preprocessing to text column\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)  # Replace 'text' with actual text column name\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e997e462",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Extraction with TF-IDF\n",
    "\n",
    "We will convert the cleaned text into numerical features using TF-IDF vectorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b0e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(data['cleaned_text']).toarray()\n",
    "y = data['label']  # Replace 'label' with the actual target column\n",
    "\n",
    "# Split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff0765",
   "metadata": {},
   "source": [
    "\n",
    "## Model Training and Evaluation\n",
    "\n",
    "We will train and evaluate multiple machine learning models and compare their performances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca59173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Performance evaluation\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_log_reg), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix: Logistic Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ae7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Performance evaluation\n",
    "print(\"Random Forest Performance:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('Confusion Matrix: Random Forest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d64a102",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion and Insights\n",
    "\n",
    "By comparing the models based on their evaluation metrics, we can determine the best-performing algorithm for detecting hate speech and offensive language. Future work may involve using deep learning models for further improvement.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
